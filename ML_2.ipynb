{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJrfa91c10dDVR4FxFrQzn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **BACK PROPOGATION**"],"metadata":{"id":"i3Hu_sDawstj"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W1_JCzpuk5l4","executionInfo":{"status":"ok","timestamp":1716436415506,"user_tz":-330,"elapsed":1173,"user":{"displayName":"DEVANSH TIWARI","userId":"10006041717618092088"}},"outputId":"68cde891-fe75-4761-fa7e-67483b8effea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 0.3246585814644244\n","Epoch 1000, Loss: 0.24058938031979332\n","Epoch 2000, Loss: 0.19602967839812668\n","Epoch 3000, Loss: 0.12066327133528423\n","Epoch 4000, Loss: 0.030459012851464524\n","Epoch 5000, Loss: 0.012541122692477976\n","Epoch 6000, Loss: 0.007368479046732715\n","Epoch 7000, Loss: 0.005092638874192999\n","Epoch 8000, Loss: 0.0038468705667606255\n","Epoch 9000, Loss: 0.0030711754089857\n","Final weights and biases:\n","wh: [[3.79198478 5.81661184]\n"," [3.80004873 5.8545897 ]]\n","bh: [[-5.82020057 -2.46277158]]\n","wout: [[-8.32186051]\n"," [ 7.66063503]]\n","bout: [[-3.45550373]]\n","Predicted output: [[0.05322146]\n"," [0.95171535]\n"," [0.95160449]\n"," [0.05175396]]\n"]}],"source":["import numpy as np\n","\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    return x * (1 - x)\n","\n","X = np.array([[0, 0],\n","              [0, 1],\n","              [1, 0],\n","              [1, 1]])\n","\n","y = np.array([[0],\n","              [1],\n","              [1],\n","              [0]])\n","\n","np.random.seed(42)\n","\n","input_layer_neurons = X.shape[1]\n","hidden_layer_neurons = 2\n","output_neurons = 1\n","\n","wh = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n","bh = np.random.uniform(size=(1, hidden_layer_neurons))\n","wout = np.random.uniform(size=(hidden_layer_neurons, output_neurons))\n","bout = np.random.uniform(size=(1, output_neurons))\n","\n","lr = 0.1\n","\n","for epoch in range(10000):\n","    hidden_layer_input = np.dot(X, wh) + bh\n","    hidden_layer_activations = sigmoid(hidden_layer_input)\n","    output_layer_input = np.dot(hidden_layer_activations, wout) + bout\n","    predicted_output = sigmoid(output_layer_input)\n","\n","    error = y - predicted_output\n","    d_predicted_output = error * sigmoid_derivative(predicted_output)\n","\n","    error_hidden_layer = d_predicted_output.dot(wout.T)\n","    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_activations)\n","\n","    wout += hidden_layer_activations.T.dot(d_predicted_output) * lr\n","    bout += np.sum(d_predicted_output, axis=0, keepdims=True) * lr\n","    wh += X.T.dot(d_hidden_layer) * lr\n","    bh += np.sum(d_hidden_layer, axis=0, keepdims=True) * lr\n","\n","    if epoch % 1000 == 0:\n","        loss = np.mean(np.square(y - predicted_output))\n","        print(f'Epoch {epoch}, Loss: {loss}')\n","\n","print(\"Final weights and biases:\")\n","print(\"wh:\", wh)\n","print(\"bh:\", bh)\n","print(\"wout:\", wout)\n","print(\"bout:\", bout)\n","print(\"Predicted output:\", predicted_output)"]}]}